{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantizing MobileNetV2...\n",
      "Calibrating MobileNetV2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calibrating:  13%|█▎        | 99/782 [04:34<31:31,  2.77s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating MobileNetV2 (INT8 Quantized)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 782/782 [26:38<00:00,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2 INT8 - Top-1 Accuracy: 0.12%, Top-5 Accuracy: 0.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torchvision.models.mobilenetv2 import InvertedResidual\n",
    "\n",
    "# Import quantization modules\n",
    "from torch.quantization import get_default_qconfig, QConfig\n",
    "from torch.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "from torch.ao.quantization import QConfigMapping\n",
    "from torch.quantization.observer import default_per_channel_weight_observer\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct_top1 = 0\n",
    "    correct_top5 = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Evaluating\", leave=True):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Top-1 accuracy\n",
    "            _, predicted_top1 = outputs.topk(1, dim=1)\n",
    "            correct_top1 += (predicted_top1.squeeze() == labels).sum().item()\n",
    "\n",
    "            # Top-5 accuracy\n",
    "            _, predicted_top5 = outputs.topk(5, dim=1)\n",
    "            correct_top5 += sum([1 if label in pred else 0 for label, pred in zip(labels, predicted_top5)])\n",
    "            total += labels.size(0)\n",
    "\n",
    "    top1_acc = 100 * correct_top1 / total\n",
    "    top5_acc = 100 * correct_top5 / total\n",
    "    return top1_acc, top5_acc\n",
    "\n",
    "data_dir = \"..\"  # Update this to your ImageNet validation dataset path\n",
    "batch_size = 64\n",
    "num_workers = 4  # Adjust based on your system\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_dataset = datasets.ImageFolder(root=f\"{data_dir}/val\", transform=transform)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Use CPU or mps device for evaluation; quantization is performed on CPU\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Define the models to test\n",
    "models_to_test = {\n",
    "    # \"ResNet18\": models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1),\n",
    "    \"MobileNetV2\": models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1),\n",
    "    # \"ViT_B_16\": models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "}\n",
    "\n",
    "for model_name, model in models_to_test.items():\n",
    "    # print(f\"\\nEvaluating {model_name} (FP32)...\")\n",
    "    # model = model.to(device)\n",
    "    # top1_fp32, top5_fp32 = evaluate_model(model, val_loader, device)\n",
    "    # print(f\"{model_name} FP32 - Top-1 Accuracy: {top1_fp32:.2f}%, Top-5 Accuracy: {top5_fp32:.2f}%\")\n",
    "\n",
    "    # Quantize the model\n",
    "    print(f\"\\nQuantizing {model_name}...\")\n",
    "    model.eval()\n",
    "    model.cpu()  # Quantization is performed on CPU\n",
    "\n",
    "    # Get default qconfig based on backend\n",
    "    torch.backends.quantized.engine = 'qnnpack'\n",
    "    default_qconfig = get_default_qconfig(torch.backends.quantized.engine)\n",
    "    # Create a per-channel qconfig\n",
    "    qconfig = QConfig(activation=default_qconfig.activation, weight=default_per_channel_weight_observer)\n",
    "    # Create QConfigMapping and set global qconfig\n",
    "    qconfig_mapping = QConfigMapping().set_global(qconfig)\n",
    "    # Exclude InvertedResidual blocks\n",
    "    qconfig_mapping.set_object_type(InvertedResidual, None)\n",
    "\n",
    "    def is_depthwise_conv(module):\n",
    "        return isinstance(module, torch.nn.Conv2d) and module.groups == module.in_channels\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if is_depthwise_conv(module):\n",
    "            qconfig_mapping.set_module_name(name, None)\n",
    "\n",
    "    # Prepare the model for quantization\n",
    "    example_inputs = torch.randn(1, 3, 224, 224)\n",
    "    try:\n",
    "        prepared_model = prepare_fx(model, qconfig_mapping, example_inputs)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to prepare {model_name} for quantization: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Calibrate the model (using a subset of data for speed)\n",
    "    print(f\"Calibrating {model_name}...\")\n",
    "    calibration_batches = 782  # Number of batches to use for calibration\n",
    "    batch_count = 0\n",
    "    with torch.no_grad():\n",
    "        for images, _ in tqdm(val_loader, desc=\"Calibrating\", leave=True):\n",
    "            prepared_model(images)\n",
    "            batch_count += 1\n",
    "            if batch_count >= calibration_batches:\n",
    "                break\n",
    "\n",
    "    # Convert the model to a quantized version\n",
    "    try:\n",
    "        quantized_model = convert_fx(prepared_model)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to convert {model_name} to quantized model: {e}\")\n",
    "        continue\n",
    "\n",
    "    quantized_model.to('cpu')\n",
    "\n",
    "    # Evaluate the quantized model\n",
    "    print(f\"\\nEvaluating {model_name} (INT8 Quantized)...\")\n",
    "    top1_int8, top5_int8 = evaluate_model(quantized_model, val_loader, 'cpu')\n",
    "    print(f\"{model_name} INT8 - Top-1 Accuracy: {top1_int8:.2f}%, Top-5 Accuracy: {top5_int8:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
